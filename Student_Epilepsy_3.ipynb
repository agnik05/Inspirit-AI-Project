{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Student Epilepsy_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqwQhAi2Zvel"
      },
      "source": [
        "# Detecting Epileptic Seizures through EEG Data: Part 3\n",
        "\n",
        "So far, we have used KNNs, logistic regression, simple NNs and CNNs for epileptic seizure detection from EEG. Since EEG is a time-series, let's explore how to use time-series-specific model, recurrent neural networks, to detect seizures!\n",
        "\n",
        "## Goals for today:\n",
        "1. Seizure detection using a simple RNN\n",
        "2. Seizure detection using a Long Short Term Memory (LSTM) network\n",
        "3. Many-to-many sequence modeling\n",
        "4. Visualizing model confidence over time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIjO4zXKZvCt",
        "cellView": "form"
      },
      "source": [
        "#@title ##Import libraries!\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, SimpleRNN, LSTM\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import keras.optimizers as optimizers\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "monitor = ModelCheckpoint('./model.hdf5', \n",
        "                          monitor='val_accuracy', \n",
        "                          verbose=0, \n",
        "                          save_best_only=True, \n",
        "                          save_weights_only=False, \n",
        "                          mode='auto', \n",
        "                          save_freq='epoch')\n",
        "\n",
        "import gdown\n",
        "\n",
        "## Set random seed for reproducible results\n",
        "RAND_SEED = 12\n",
        "np.random.seed(RAND_SEED)\n",
        "tf.random.set_seed(RAND_SEED)\n",
        "\n",
        "## Utils function to combine 23 chunks from the same patient into one big chunk\n",
        "def prepare_data(eeg_df):\n",
        "  file_names = eeg_df['Unnamed: 0'].tolist()\n",
        "\n",
        "  subject_ids = []\n",
        "  chunk_ids = []\n",
        "  for fn in file_names:\n",
        "    subject_ids.append(fn.split('.')[-1])\n",
        "    chunk_ids.append(fn.split('.')[0])\n",
        "  subject_ids = list(set(subject_ids))\n",
        "  assert len(subject_ids) == 500\n",
        "\n",
        "  sub2ind = {}\n",
        "  for ind, sub in enumerate(subject_ids):\n",
        "    sub2ind[sub] = ind\n",
        "\n",
        "  eeg_combined = np.zeros((500, int(178*23)))\n",
        "  labels_combined = np.zeros(500)\n",
        "  labels_chunks = np.zeros((500, 23))\n",
        "  labels_dict = {}\n",
        "  for i in range(len(eeg_df)):\n",
        "    fn = eeg_df.iloc[i]['Unnamed: 0']\n",
        "    subject_id = fn.split('.')[-1]\n",
        "    subject_ind = sub2ind[subject_id]\n",
        "\n",
        "    chunk_id = int(fn.split('.')[0].split('X')[-1])\n",
        "    start_idx = (chunk_id - 1) * 178\n",
        "    end_idx = start_idx + 178\n",
        "    eeg_combined[subject_ind, start_idx:end_idx] = eeg_df.iloc[i].values[1:-1]\n",
        "\n",
        "    if subject_id not in labels_dict:\n",
        "      labels_dict[subject_id] = []\n",
        "    labels_dict[subject_id].append(eeg_df.iloc[i].values[-1])\n",
        "\n",
        "  for sub_id, labels in labels_dict.items():\n",
        "    sub_ind = sub2ind[sub_id]\n",
        "    is_seizure = int(np.any(np.array(labels) == 1))\n",
        "    labels_combined[sub_ind] = is_seizure\n",
        "    labels = np.array(labels)\n",
        "    labels = np.where(labels>1, 0, labels)\n",
        "    labels_chunks[sub_ind,:] = labels\n",
        "\n",
        "  return eeg_combined, labels_combined, labels_chunks\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-4sm1qQ7PFF",
        "cellView": "form"
      },
      "source": [
        "#@title ## Download our data set!\n",
        "data_path = 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/Deep%20Dives/AI%20%2B%20Healthcare/Projects%20(Session%206%2B)/Seizure%20Prediction%20/data.csv'\n",
        "uci_epilepsy = './uci_epilepsy'\n",
        "gdown.download(data_path, uci_epilepsy, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pe8r5UmtkGf"
      },
      "source": [
        "# Let's revisit the EEG data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_hHKe9kzrKK"
      },
      "source": [
        "eeg = pd.read_csv(uci_epilepsy)\n",
        "eeg.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZbkHIui1g6e"
      },
      "source": [
        "## Visualizing one EEG sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0pbn3laiKxd"
      },
      "source": [
        "eeg_all, _, _ = prepare_data(eeg)\n",
        "\n",
        "eeg1 = np.array(eeg_all[0,:]) # take the first EEG\n",
        "print('Length of one EEG sequence: ', len(eeg1))\n",
        "print(eeg1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj_3AC5Ym6Ta"
      },
      "source": [
        "x = np.linspace(0, 23, 4094)\n",
        "\n",
        "plt.plot(x,eeg1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWJo4dnP2uA-"
      },
      "source": [
        "#### **Question**: What are the x-axis and y-axis here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YBCyt1s3hDt"
      },
      "source": [
        "## Prepare input data for RNNs\n",
        "\n",
        "RNNs are designed for **sequence data**: the model processes its input and produces output **in order**. \n",
        "\n",
        "Check out [these slides](https://docs.google.com/presentation/d/1ykLNZNkql0SDqqDiNLKJVspUFhVLKB_7x-uB7sSlbNI/) for more.\n",
        "\n",
        "The \"vanilla\" version of RNNs works as follows. As a vanilla RNN processes the input, it keeps a **hidden state vector** to remember some of what it's seen so far. At each step with each **new input**, the model uses the **old hidden state** to compute a **new hidden state** and a **new output**.\n",
        "\n",
        "For our seizure detection task, the input is an EEG sequence, and the output is one single label indicating seizure or no seizure. Therefore, we can use the **final hidden state** as the representation for the whole sequence, and feed it to fully connected layer(s) for seizure detection! This is often called **many-to-one** sequence modeling.\n",
        "\n",
        "In our dataset, each sample consists of 23 seconds of EEG recordings, and each second has 178 sampled data points. Therefore, we can treat one second as one **time step**, and thus each time step is a vector of length 178.\n",
        "\n",
        "[//]: <> (Here, we will treat each of the 178 data points as one **time step**, and thus each time step is simply a continuous number, i.e. the feature dimension is 1.)\n",
        "\n",
        "An RNN requires its input to have shape `(number of time steps, feature dimension)`. Let's convert `eeg1` into the required shape. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsIbEN-S-VfT"
      },
      "source": [
        "# Convert eeg1 to the input shape required by an RNN\n",
        "print('eeg1 shape before reshape: ', eeg1.shape)\n",
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE\n",
        "print('eeg1 shape after reshape: ', eeg1.shape) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoEaeRflzY82"
      },
      "source": [
        "# Build and Evaluate a vanilla RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we1xNYzY3aVp"
      },
      "source": [
        "### First, extract input and output data\n",
        "\n",
        "Our input data `x` are EEG sequences, and output data `y` are binary labels indicating seizure or no seizure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6giFCiVUzOCZ"
      },
      "source": [
        "# Get x (EEG data) and y (binary classes --- 1 for seizure and 0 for no seizure)\n",
        "# y_time_steps here has one label for each time step, will be used later\n",
        "x, y, y_time_steps = prepare_data(eeg)\n",
        "x = x.reshape(-1, 23, 178).astype(np.float32) # reshape x into (number_of_samples, number_of_time_steps, feature dimension)\n",
        "\n",
        "y = y.astype(int).reshape(-1,1) # reshape y into (num_of_samples, 1)\n",
        "\n",
        "print('Input x shape: ', x.shape)\n",
        "print('Label y shape: ', y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI-xPorfpU0g"
      },
      "source": [
        "Before we split the data into train and test sets, let's look at how many samples have seizures and how many samples do not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-mqzfh_pNjI"
      },
      "source": [
        "# Count #seizures vs #non-seizures in y\n",
        "# Remember that 1 means seizure and 0 means non-seizure\n",
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p7pQ6a8qT_O"
      },
      "source": [
        "### Discussion: Imbalanced Data\n",
        "\n",
        "We can see that our data is imbalanced, i.e. there are much more non-seizure cases than seizure cases. Imbalanced data is common in healthcare domains --- there are usually more healthy cases than diseased cases.\n",
        "\n",
        "Would this be a potential problem for machine learning models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gnko1d0U6SEm"
      },
      "source": [
        "### Exercise: Split the data into training and test sets\n",
        "Split the data into training and test sets. Make the training vs test set ratio = `80%/20%`. You may set `random_state=1` for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4bblpvN9X8H"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE\n",
        "\n",
        "print('x_train shape: ', x_train.shape)\n",
        "print('y_train shape: ', y_train.shape)\n",
        "print('x_test shape: ', x_test.shape)\n",
        "print('y_test shape: ', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0aTZ8nu-ZdJ"
      },
      "source": [
        "Count the number of seizures vs non-seizures in training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLsO2tRZ-hT6"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE\n",
        "\n",
        "print('Number of seizures in train set: ', num_seizure_train)\n",
        "print('Number of non-seizures in train set: ', num_nonseizure_train)\n",
        "\n",
        "print('Number of seizures in test set: ', num_seizure_test)\n",
        "print('Number of non-seizures in test set: ', num_nonseizure_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAgEuQ3m5-E_"
      },
      "source": [
        "### Let's build a vanilla RNN!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "811X0vsfCa7Q"
      },
      "source": [
        "Let's build an RNN with the following architecture:\n",
        "\n",
        "*   2 RNN layers with 32 RNN units\n",
        "*   1 fully connected (i.e. dense) layer\n",
        "\n",
        "You may find the following Keras functions useful (already imported):\n",
        "*   `SimpleRNN(units = number_of_rnn_units, return_sequences = True)`\n",
        "*   `SimpleRNN(units = number_of_rnn_units)`\n",
        "*   `Dense(units = number_of_output_neurons, activation = 'sigmoid')`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZsFJOpjEqvy"
      },
      "source": [
        "# Define an RNN with the 2 RNN layers and one fully connected layer\n",
        "\n",
        "rnn = Sequential()\n",
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIirsa1BF2Wn"
      },
      "source": [
        "Compile and train the RNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrtDZ2Z8FZRQ"
      },
      "source": [
        "# Compile the RNN\n",
        "rnn.compile(loss='binary_crossentropy',\n",
        "            optimizer = 'adam', \n",
        "            metrics = ['accuracy'])\n",
        "\n",
        "# Train the RNN\n",
        "rnn.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, callbacks=[monitor])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unXEXn7eN5zL"
      },
      "source": [
        "Predict on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv3ztnk9Kyvq"
      },
      "source": [
        "# Predict on the test data\n",
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE\n",
        "\n",
        "print('Test accuracy: ', accuracy_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zETWQyVUUU0o"
      },
      "source": [
        "### Exercise:\n",
        "Try **adding more RNN layers** and/or **varying the number of RNN units** and observe the model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swHMXge0O6Pu"
      },
      "source": [
        "## Long Short Term Memory (LSTM)\n",
        "The performance of vanilla RNNs often degrades when the input sequence is long because\n",
        "* The activation function in RNNs is non-linear --- information gets zeroed out and washed away for long sequences.\n",
        "*  Vanilla RNNs are **not selective** when it comes to **what to keep** and **what to forget** --- all information is read and all information is overwritten.\n",
        "\n",
        "Hence, we need models that can choose what to keep and what to forget! LSTMs allow us to do that.\n",
        "\n",
        "LSTM is a special type of RNN that is designed to learn **long-term dependencies**. They were first introduced by Hochreiter & Schmidhuber in 1997 ([paper](https://www.bioinf.jku.at/publications/older/2604.pdf)).\n",
        "\n",
        "LSTMs have **two** forms of memory --- **cell state** and **hidden state**. LSTMs control the information flow into and out of the cell through the three **gates** --- **input, forget** and **output gates**.\n",
        "\n",
        "A schematic of the LSTM cell is shown below:\n",
        "![LSTM](https://drive.google.com/uc?id=17b7oxigMmGcyx19fyzCKF-pf6dIBMLVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO-biwwwTnPa"
      },
      "source": [
        "Now, let's see if we can further improve the seizure detection accuracy with a LSTM! \n",
        "\n",
        "Our LSTM has 2 LSTM layers with 32 units and 1 fully connected layer. The following Keras functions may be useful:\n",
        "*  `LSTM(units = number_of_rnn_units)`\n",
        "*  `Dense(units = number_of_output_neurons, activation = 'sigmoid')`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oOnURPIVFDV"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "# Build a LSTM with 2 LSTM layers and one fully connected layer\n",
        "\n",
        "# Compile the model\n",
        "\n",
        "# Train the model\n",
        "\n",
        "# Predict on test data\n",
        "\n",
        "### END CODE\n",
        "print('Test accuracy: ', accuracy_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnRcQ__IXTCC"
      },
      "source": [
        "Woohoo! Higher test accuracy using LSTM!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwsjEYp9tzNQ"
      },
      "source": [
        "## Many-to-many Sequence Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ8YAWo1Furk"
      },
      "source": [
        "So far, we have seen **many-to-one** sequence modeling using a vanilla RNN and a LSTM.\n",
        "\n",
        "In addition to many-to-one modeling, RNNs are also capable of **many-to-many** modeling --- we can get an output from RNN at each time step.\n",
        "\n",
        "Now, let's build a many-to-many LSTM with 2 LSTM layers and one fully connected layer with sigmoid activation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx0yKyl3MlvM"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Build a many-to-many LSTM with 2 LSTM layers (32 RNN units) and one fully connected layer with sigmoid activation.\n",
        "\n",
        "**Hint:** The keras LSTM layer has an argument `return_sequences`. Setting it to be `True` will give you output at each time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UYH-szDLpwp"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "# Build a LSTM with 2 LSTM layers and one fully connected layer\n",
        "\n",
        "# Compile the model\n",
        "\n",
        "# Train the model\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmbHAqFmMuFc"
      },
      "source": [
        "# Get predicted probabilities on test set\n",
        "proba = lstm_many2many.predict(x_test)\n",
        "print('proba shape: ', proba.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJvPSKxvM6K1"
      },
      "source": [
        "**Question:**  Can you explain the shape of `proba`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRiWIPafFOyb"
      },
      "source": [
        "In our dataset, for each of the 500 samples, in addition to one combined seizure/non-seizure label for 23 seconds (i.e. 23 time steps), we also have fine-grained labels for each time step. The labels are saved in `y_time_steps`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95S36eZqtyPe"
      },
      "source": [
        "# Get labels for each time step in test set\n",
        "y_test_time_steps = y_time_steps[test_indices]\n",
        "print(y_test_time_steps.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlEMhuBbNuhv"
      },
      "source": [
        "## **(Advanced)** Model Explainability --- Visualizing Model Confidence Over Time\n",
        "\n",
        "Now that we have the model's predicted probabilities for each time step, we can visualize the model's confidence over time, and compare it to the ground truth labels over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f11rWoDN9nI"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Write a function to visualize the model's predicted probabilities and the labels over time for one sample in the same plot.\n",
        "\n",
        "Use red color for probabilities and blue color for labels. You may find the function `plt.plot` useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKnvtGJqtyLA"
      },
      "source": [
        "def viz_model_conf_over_time(proba, label):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    proba: list of predicted probabilities for each time step for one sample, shape (23,)\n",
        "    label: list of labels for each time step for one sample, shape (23,)\n",
        "  \"\"\"\n",
        "\n",
        "  ### YOUR CODE HERE\n",
        "\n",
        "  ### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6cB_VUhRbhr"
      },
      "source": [
        "Let's randomly choose 10 examples from the test set and visualize them. Feel free to visualize more examples by modifying `range(10)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYXNlkd9tyGM"
      },
      "source": [
        "# Randomly choose 10 samples from the test set and visualize them\n",
        "for _ in range(10):\n",
        "  rand_index = np.random.randint(len(y_test))\n",
        "  print(rand_index)\n",
        "  viz_model_conf_over_time(proba[rand_index], y_test_time_steps[rand_index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZDbd01_Sw_k"
      },
      "source": [
        "### Discussion\n",
        "\n",
        "From the above visualization,\n",
        "* How's our model performing in general?\n",
        "* What is the trend in the model's confidence over time?\n",
        "* Do you see any mistake made by the model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOYjNdANwA3Y"
      },
      "source": [
        "## Final Remarks\n",
        "**Congratulations!** You have successfully completed the project!\n",
        "\n",
        "Our models achieved >90% accuracy. Are they good enough for deployment to hospitals to assist the doctors? Most likely not. \n",
        "\n",
        "The real-world scenario is much more complicated. The seizure EEGs we use here are from single EEG channels (i.e. electrodes) that have been annotated as seizures. In clinical settings, EEGs are usually recorded from multiple EEG electrodes. Our models need to be able to detect seizures from multi-channel EEGs without being explicitly given the single channel data.\n",
        "\n",
        "Moreover, the dataset we use here is from a single institution. EEGs from a different institution might be very different from what we have seen here. **Can our models generalize to data from new institutions?** In fact, making AI models clinically applicable and generalizable to unseen data is still an open question and an active field of research.\n",
        "\n",
        "Finally, deploying medical AI models faces many other issues, one of which is **ethics**. For example, how to ensure patients' privacy? Should a doctor fully trust an AI model's decisions? If there is a mis-diagnosis by the AI model, who should take the responsibility? \n",
        "\n",
        "Nevertheless, do not let the challenges stop us. We are extremely excited about the future of AI in healthcare!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRoi2KwNVhJM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}